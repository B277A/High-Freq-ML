{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developmental-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool\n",
    "from pandas.tseries.offsets import MonthEnd, YearEnd\n",
    "import scipy.io\n",
    "import wrds\n",
    "from functools import reduce\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import datetime as dt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-madison",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "downtown-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main directory\n",
    "main_folder = '../../../../GitHub/High-Freq-ML'\n",
    "\n",
    "# Folders are relative to main directory\n",
    "crsp_folder = f'{main_folder}/data/crsp/daily/'\n",
    "compustat_folder = f'{main_folder}/data/compustat/daily/'\n",
    "taq_price_folder = f'{main_folder}/data/taq/prices/'\n",
    "output_folder = f'{main_folder}/data/proc/clean_prices_etfs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-illustration",
   "metadata": {},
   "source": [
    "# Identify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pharmaceutical-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of TAQ files\n",
    "taq_price_files = glob.glob(taq_price_folder + '*.parquet')\n",
    "taq_price_files_dates = [x.split('/')[-1].split('_')[0].split('.')[0] for x in taq_price_files]\n",
    "taq_price_files_dates = list(set(taq_price_files_dates))\n",
    "\n",
    "# Get list of CRSP files\n",
    "crsp_files = glob.glob(crsp_folder + '*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970d547-821a-4b52-b984-1ca9ec90a1e1",
   "metadata": {},
   "source": [
    "# Start logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78d1671-018d-4052-8c7e-a21734b324d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare logging file\n",
    "log_filename = \"clean_prices.log\"\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "\n",
    "# Logging configuration\n",
    "formatter = logging.Formatter(\n",
    "    \"[{asctime}] — [{funcName:12.12}] — [{levelname:<8s} — Line {lineno:4d}]: {message}\",\n",
    "    style=\"{\",\n",
    ")\n",
    "# logging.basicConfig(stream=None, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "handler = logging.FileHandler(filename=log_filename)\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-matrix",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f81c841-f3e3-419d-a59c-ee3bf8ae98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Stock info\n",
    "\n",
    "# # For reuse\n",
    "# stock_info_df = pd.read_feather('../../data/keys/stock_universe.feather')\n",
    "# stock_info_df['jdate'] = pd.to_datetime(stock_info_df['dt'].dt.date) + MonthEnd(0)\n",
    "# stock_info_df['jdate_ym'] = stock_info_df['jdate'].dt.strftime('%Y%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-brighton",
   "metadata": {},
   "source": [
    "## Delisting Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "experienced-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## Delisting returns\n",
    "conn = wrds.Connection(**{\"wrds_username\": \"sa400\"})\n",
    "crspmsedelist_df = conn.raw_sql(\"\"\"\n",
    "                    select DLSTDT, PERMNO, dlret\n",
    "                   from crsp.msedelist\n",
    "                    \"\"\")\n",
    "crspmsedelist_df['date'] = pd.to_datetime(crspmsedelist_df['dlstdt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-albert",
   "metadata": {},
   "source": [
    "## Lagged ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smooth-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.11 s, sys: 12.8 s, total: 17.9 s\n",
      "Wall time: 8.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crsp_me_df = pd.read_feather(f'{main_folder}/data/keys/crsp_me.feather')\n",
    "crsp_me_df['permno'] = crsp_me_df['permno'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-strip",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-kingston",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desperate-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crsp(yyyymm):\n",
    "    \n",
    "    # Read in data\n",
    "    crsp_df = pd.concat([pd.read_parquet(x) for x in crsp_files if f'/{yyyymm}' in x],\n",
    "                       ignore_index = True)\n",
    "\n",
    "    # Clean up columns\n",
    "    crsp_df.columns = [x.lower() for x in crsp_df.columns]\n",
    "    crsp_df[['ret', 'retx']] = crsp_df[['ret', 'retx']].apply(pd.to_numeric, errors = 'coerce')\n",
    "\n",
    "    # Fix dates\n",
    "    crsp_df['date'] = pd.to_datetime(crsp_df['date'], format = '%Y%m%d')\n",
    "\n",
    "    # Get close and open prices\n",
    "    crsp_df['prc'] = np.abs(crsp_df['prc'])\n",
    "    crsp_df['openprc'] = np.abs(crsp_df['openprc']).fillna(crsp_df['prc'])\n",
    "\n",
    "    # Catch nonsensical open prices: rule is to find cases where close price is \n",
    "    # 5 times larger than open price while close-to-close return  is less than \n",
    "    # 90% in magnitude\n",
    "    crsp_df = crsp_df.sort_values(by = 'date')\n",
    "    crsp_df['prc_lead'] = crsp_df.groupby(['permno'])['prc'].shift(-1)\n",
    "    crsp_df.loc[\n",
    "        crsp_df.query(\"prc/openprc > 5 & abs(ret) < 0.90\").index, \"openprc\"\n",
    "    ] = crsp_df.loc[crsp_df.query(\"prc/openprc > 5 & abs(ret) < 0.90\").index, \"prc\"]\n",
    "\n",
    "    # Infer close-to-open adjusted overnight returns \n",
    "    crsp_df['ret_open_close_intraday'] = (crsp_df['prc']-crsp_df['openprc'])/crsp_df['openprc']\n",
    "    crsp_df['ret_close_open_adj'] = (1+crsp_df['ret'])/(1+crsp_df['ret_open_close_intraday']) - 1\n",
    "    crsp_df['retx_close_open_adj'] = (1+crsp_df['retx'])/(1+crsp_df['ret_open_close_intraday']) - 1\n",
    "\n",
    "    # Add lagged market equity\n",
    "    crsp_me_subset_df = crsp_me_df.loc[crsp_me_df['date'].between(\n",
    "        a:=pd.to_datetime(yyyymm, format = '%Y%m'), a + MonthEnd(1))]\n",
    "    crsp_df = crsp_df.merge(crsp_me_subset_df, on = ['permno', 'date'], how = 'left')\n",
    "\n",
    "    # Filter by share/exchange code and whether stock is primary\n",
    "    proc_df = crsp_df.copy()\n",
    "\n",
    "    # Hand-cleaning, no easy way to deal with these 'bad' permnos, so just drop\n",
    "    proc_df = proc_df.loc[~proc_df['permno'].isin([90806, 83712, 47387])]\n",
    "    if int(yyyymm[:4]) < 2018:\n",
    "        proc_df = proc_df.loc[~proc_df['permno'].isin([15293])]\n",
    "    if int(yyyymm) == 200301:\n",
    "        proc_df = proc_df.loc[~proc_df['permno'].isin([87658])]\n",
    "\n",
    "    # Create dataframes for start and end of the day\n",
    "    crsp_df_start = proc_df.copy()\n",
    "    crsp_df_end = proc_df.copy()\n",
    "    crsp_df_end = crsp_df_end.merge(crspmsedelist_df, on = ['date', 'permno'], how = 'left')\n",
    "\n",
    "    crsp_df_start['time'] = '09:30:00'\n",
    "    crsp_df_end['time'] = '16:00:00'\n",
    "\n",
    "    crsp_df_start['price'] = crsp_df_start['openprc']\n",
    "    crsp_df_end['price'] = crsp_df_end['prc']\n",
    "\n",
    "    crsp_df_start['return'] = crsp_df_start['ret_close_open_adj']\n",
    "    crsp_df_end['return'] = (1+crsp_df_end['ret_open_close_intraday'].fillna(0))*(1+crsp_df_end['dlret'].fillna(0))-1\n",
    "\n",
    "    crsp_df_start['returnx'] = crsp_df_start['retx_close_open_adj']\n",
    "    crsp_df_end['returnx'] = crsp_df_end['ret_open_close_intraday']\n",
    "\n",
    "    crsphf_df = pd.concat([crsp_df_start, crsp_df_end], ignore_index = True)\n",
    "\n",
    "    # Add datetime info\n",
    "    crsphf_df['datetime'] = crsphf_df['date'] + pd.to_timedelta(crsphf_df['time'])\n",
    "    crsphf_df['yyyymm'] = crsphf_df['date'].dt.strftime('%Y%m').astype(int)    \n",
    "\n",
    "    return crsphf_df[['datetime', 'date', 'permno', 'cusip', 'price', 'return', 'returnx', 'dlret', 'meq_close_lag', 'me_close_lag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subject-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all times to include\n",
    "all_times = [\n",
    "    (pd.to_datetime(\"09:30:00\") + pd.Timedelta(f\"{x*5}min\")).strftime(\"%H:%M:%S\")\n",
    "    for x in range(0, 79)\n",
    "]\n",
    "all_times_taq = [x.replace(\"09\", \"9\") for x in all_times]\n",
    "\n",
    "\n",
    "def clean_taq(date):\n",
    "\n",
    "    # Get TAQ files\n",
    "    taq_df = pd.read_parquet(\n",
    "        taq_price_folder + date + \".parquet\", columns=[\"permno\", \"symbol\", \"date\", \"time\", \"price\"]\n",
    "    )\n",
    "\n",
    "    # Drop \"when issued\" shares\n",
    "    taq_df = (\n",
    "        taq_df.assign(symbol_last_2=taq_df[\"symbol\"].str.slice(-2))\n",
    "        .query('symbol_last_2 != \"WI\"')\n",
    "        .drop(\"symbol_last_2\", axis=1)\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # Drop cases with missing permnos\n",
    "    taq_df = taq_df.loc[taq_df[\"permno\"].str.isnumeric()]\n",
    "    taq_df[\"permno\"] = pd.to_numeric(taq_df[\"permno\"]).astype(int)\n",
    "\n",
    "    # Handle any missing times\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [taq_df[\"permno\"].unique(), all_times_taq], names=[\"permno\", \"time\"]\n",
    "    )\n",
    "    index_df = pd.DataFrame(index=index).reset_index()\n",
    "    taq_df = (\n",
    "        taq_df.merge(index_df, on=[\"permno\", \"time\"], how=\"right\")\n",
    "        .sort_values(by=[\"permno\"])\n",
    "        .astype({\"time\": \"category\"})\n",
    "    )\n",
    "    taq_df = taq_df.sort_values(by=[\"permno\", \"time\"])\n",
    "    \n",
    "    # Catch scenario where there are less than full set of \n",
    "    # prices in day (Thanksgiving, Christmas, etc)\n",
    "    valid_times = taq_df[['time', 'price']].dropna(axis=0)['time'].unique()\n",
    "    if len(valid_times) < len(all_times):\n",
    "        logging.warning(f'[{date}] This date has only {len(valid_times)} valid times')\n",
    "\n",
    "    taq_df = taq_df.loc[taq_df['time'].isin(list(valid_times))]\n",
    "\n",
    "    # Forward fill in entries\n",
    "    ffill_cols = [\"price\"]  # 'cusip9', 'symbol', 'ticker_identifier'\n",
    "    taq_df[ffill_cols] = taq_df.groupby([\"permno\"])[ffill_cols].ffill()\n",
    "    taq_df[\"date\"] = int(date)\n",
    "\n",
    "    # Add date\n",
    "    taq_df[\"datetime\"] = pd.to_datetime(taq_df[\"date\"], format=\"%Y%m%d\") + pd.to_timedelta(\n",
    "        taq_df[\"time\"]\n",
    "    )\n",
    "    \n",
    "    # Drop cases where realized quarticity for the day is extremely high, just a \n",
    "    # crude way of catching very bad data\n",
    "    taq_df = taq_df.sort_values(by=[\"permno\", \"datetime\"])\n",
    "    taq_df['price_pct_change'] = taq_df.groupby(['permno'])['price'].transform('pct_change').fillna(0)\n",
    "    taq_df[\"log_ret_4\"] = np.power(np.log(1 + taq_df[\"price_pct_change\"]), 4)\n",
    "    taq_df = taq_df[\n",
    "        ~taq_df[\"permno\"].isin(\n",
    "            pd.DataFrame(taq_df.groupby([\"permno\"])[\"log_ret_4\"].sum() > 1).query(\"log_ret_4\").index\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Sort\n",
    "    taq_df = taq_df.sort_values(by=[\"permno\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "    return taq_df[[\"permno\", \"datetime\", \"price\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moved-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_weights(rm_df):\n",
    "\n",
    "    # Add value weights\n",
    "    rm_df = rm_df.sort_values(by=[\"permno\", \"datetime\"]).reset_index(drop=True)\n",
    "    rm_df[\"1+retx\"] = rm_df[\"returnx\"].fillna(0) + 1\n",
    "    rm_df[\"cumretx\"] = rm_df.groupby([\"permno\", \"date\"])[\"1+retx\"].cumprod()\n",
    "    rm_df[\"meq_close_lag_times_cumretx\"] = rm_df[\"meq_close_lag\"] * rm_df[\"cumretx\"]\n",
    "    rm_df[\"me_close_lag_times_cumretx\"] = rm_df[\"me_close_lag\"] * rm_df[\"cumretx\"]\n",
    "    rm_df[\"value_wt\"] = rm_df.groupby([\"permno\"])[\"meq_close_lag_times_cumretx\"].shift(1)\n",
    "    rm_df[\"value_wt\"] = rm_df[\"value_wt\"].fillna(rm_df[\"meq_close_lag\"])\n",
    "    rm_df[\"value_wt_permno\"] = rm_df.groupby([\"permno\"])[\"me_close_lag_times_cumretx\"].shift(1)\n",
    "    rm_df[\"value_wt_permno\"] = rm_df[\"value_wt\"].fillna(rm_df[\"me_close_lag\"])\n",
    "    rm_df = rm_df.drop(\n",
    "        [\n",
    "            \"1+retx\",\n",
    "            \"cumretx\",\n",
    "            \"meq_close_lag_times_cumretx\",\n",
    "            \"meq_close_lag\",\n",
    "            \"me_close_lag_times_cumretx\",\n",
    "            \"me_close_lag\",\n",
    "            \"price\",\n",
    "            \"crsp_taq_merge_indicator\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return rm_df\n",
    "\n",
    "\n",
    "def filter_bad_merges(rm_df, close_hour, close_minute):\n",
    "    \"\"\"\n",
    "    Deals with potentially incorrect merges between TAQ and CRSP\n",
    "    by checking if the prices make sense. Main check is to see if\n",
    "    the intradaily prices jump far too much at the open and close.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Deal with TAQ and CRSP mismatches\n",
    "    # Coarse procedure to catch mismatched TAQ and CRSP stocks\n",
    "    rm_df = rm_df.sort_values(by=[\"permno\", \"datetime\"])\n",
    "    rm_df[\"price_pct_change\"] = rm_df.groupby([\"permno\"])[\"price\"].pct_change()\n",
    "    rm_df[\"price_pct_change_abs\"] = np.abs(rm_df[\"price_pct_change\"])\n",
    "    rm_df.loc[\n",
    "        (rm_df[\"datetime\"].dt.time == dt.time(9, 35))\n",
    "        | (rm_df[\"datetime\"].dt.time == dt.time(close_hour, close_minute)),\n",
    "        \"price_pct_change\",\n",
    "    ] = np.nan\n",
    "    rm_df[\"price_vol\"] = rm_df.groupby([\"permno\"])[\"price_pct_change\"].transform(\"std\")\n",
    "\n",
    "    # State which stocks need to be dropped\n",
    "    drop_df = rm_df.loc[\n",
    "        (rm_df[\"price_pct_change_abs\"] ** 2 > 3 * rm_df[\"price_vol\"])\n",
    "        & (rm_df[\"price_pct_change_abs\"] > 0.05)\n",
    "        & (rm_df[\"meq_close_lag\"] > 1e5)\n",
    "    ]\n",
    "    drop_instances_df = (\n",
    "        drop_df.groupby([\"permno\"])[\"datetime\"]\n",
    "        .count()\n",
    "        .rename(\"instances\")\n",
    "        .astype(int)\n",
    "        .reset_index()\n",
    "        .query(\"instances == 2\")\n",
    "    )\n",
    "    if len(drop_instances_df):\n",
    "        logging.warning(\n",
    "            f'[{rm_df.iloc[0][\"date\"].date()}] Dropping PERMNOs {\", \".join(drop_instances_df[\"permno\"].unique().astype(str))} '\n",
    "            + \"due to potential mismatches\"\n",
    "        )\n",
    "    rm_df = rm_df.drop([\"price_pct_change\", \"price_pct_change_abs\", \"price_vol\"], axis=1)\n",
    "\n",
    "    # Drop intradaily prices for those stocks - interpolation will fill in missing later\n",
    "    rm_df.loc[\n",
    "        (rm_df[\"datetime\"].dt.time != dt.time(9, 30))\n",
    "        & (rm_df[\"datetime\"].dt.time != dt.time(close_hour, close_minute))\n",
    "        & (rm_df[\"permno\"].isin(drop_instances_df[\"permno\"].unique())),\n",
    "        \"price\",\n",
    "    ] = np.nan\n",
    "\n",
    "    return rm_df\n",
    "\n",
    "\n",
    "def merge_crsp_taq(date, clean_crsp_date_df, interpolate=True):\n",
    "\n",
    "    # Store additional info\n",
    "    log = [date]\n",
    "\n",
    "    # Get clean TAQ data\n",
    "    clean_taq_df = clean_taq(date.strftime(\"%Y%m%d\"))\n",
    "\n",
    "    # Handle any missing times\n",
    "    all_datetimes = pd.to_datetime(clean_taq_df[\"datetime\"].unique())\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [clean_crsp_date_df[\"permno\"].unique(), all_datetimes], names=[\"permno\", \"datetime\"]\n",
    "    )\n",
    "    index_df = pd.DataFrame(index=index).reset_index()\n",
    "\n",
    "    # Adjust CRSP close time to match close time based on TAQ data\n",
    "    close_hour = np.max(all_datetimes).hour\n",
    "    close_minute = np.max(all_datetimes).minute\n",
    "\n",
    "    if (close_hour != 16) or (close_minute != 0):\n",
    "        logging.warning(\n",
    "            f\" [{date.date()}] Adjusting market close time to (H={close_hour}, M={close_minute})\"\n",
    "        )\n",
    "        clean_crsp_date_df.loc[\n",
    "            clean_crsp_date_df[\"datetime\"] == clean_crsp_date_df[\"datetime\"].max(), \"datetime\"\n",
    "        ] = clean_crsp_date_df.loc[\n",
    "            clean_crsp_date_df[\"datetime\"] == clean_crsp_date_df[\"datetime\"].max(), \"datetime\"\n",
    "        ].apply(\n",
    "            lambda x: x.replace(hour=close_hour, minute=close_minute)\n",
    "        )\n",
    "\n",
    "    # Adjust CRSP open time to match close time based on TAQ data\n",
    "    open_hour = np.min(all_datetimes).hour\n",
    "    open_minute = np.min(all_datetimes).minute\n",
    "\n",
    "    if (open_hour != 9) or (open_minute != 30):\n",
    "        logging.warning(\n",
    "            f\"Adjusting market open time for {date.date()} to (H={open_hour}, M={open_minute})\"\n",
    "        )\n",
    "        clean_crsp_date_df.loc[\n",
    "            clean_crsp_date_df[\"datetime\"] == clean_crsp_date_df[\"datetime\"].min(), \"datetime\"\n",
    "        ] = clean_crsp_date_df.loc[\n",
    "            clean_crsp_date_df[\"datetime\"] == clean_crsp_date_df[\"datetime\"].min(), \"datetime\"\n",
    "        ].apply(\n",
    "            lambda x: x.replace(hour=open_hour, minute=open_minute)\n",
    "        )\n",
    "        raise NotImplementedError(\n",
    "            f\"Market open is delayed on {date.date()} to (H={open_hour}, M={open_minute}); \"\n",
    "            + \"Adjusting for this problem has not been implemented\"\n",
    "        )\n",
    "\n",
    "    # Resample the CRSP data\n",
    "    resample_df = clean_crsp_date_df.merge(index_df, on=[\"permno\", \"datetime\"], how=\"right\")\n",
    "\n",
    "    # Merge with taq\n",
    "    rm_df = resample_df.merge(\n",
    "        clean_taq_df,\n",
    "        on=[\"permno\", \"datetime\"],\n",
    "        how=\"left\",\n",
    "        suffixes=[\"_crsp\", \"_taq\"],\n",
    "        indicator=\"crsp_taq_merge_indicator\",\n",
    "    )\n",
    "    rm_df[\"crsp_taq_merge_indicator\"] = pd.Categorical(rm_df[\"crsp_taq_merge_indicator\"])\n",
    "\n",
    "    # Create merged price series\n",
    "    rm_df[\"price\"] = rm_df[\"price_crsp\"].fillna(rm_df[\"price_taq\"])\n",
    "    rm_df = rm_df.drop([\"price_crsp\", \"price_taq\"], axis=1)\n",
    "\n",
    "    ## Fix missing data\n",
    "    # Fill in missing columns\n",
    "    rm_df[\"date\"] = date\n",
    "    rm_df[\"meq_close_lag\"] = rm_df.groupby([\"permno\"])[\"meq_close_lag\"].ffill()\n",
    "    rm_df[\"me_close_lag\"] = rm_df.groupby([\"permno\"])[\"me_close_lag\"].ffill()\n",
    "\n",
    "    ## Deal with mismatches between TAQ and CRSP\n",
    "    rm_df = filter_bad_merges(rm_df, close_hour, close_minute)\n",
    "\n",
    "    ## Interpolate log prices and fill in missing prices/returns\n",
    "    # otherwise just use standard forward filling\n",
    "    if interpolate:\n",
    "        rm_df[\"log_price\"] = np.log(rm_df[\"price\"])\n",
    "        rm_df[\"log_price_last\"] = rm_df.groupby([\"permno\"])[\"log_price\"].transform(\"last\")\n",
    "        rm_df[\"log_price_first\"] = rm_df.groupby([\"permno\"])[\"log_price\"].transform(\"first\")\n",
    "        rm_df[\"interp_beta\"] = (rm_df[\"log_price_last\"] - rm_df[\"log_price_first\"]) / (\n",
    "            len(all_datetimes) - 1\n",
    "        )\n",
    "        rm_df[\"count\"] = rm_df.groupby([\"permno\"])[\"datetime\"].cumcount()\n",
    "        rm_df[\"log_price_interp\"] = rm_df[\"log_price_first\"] + rm_df[\"count\"] * rm_df[\"interp_beta\"]\n",
    "        rm_df[\"price\"] = rm_df[\"price\"].fillna(np.exp(rm_df[\"log_price_interp\"]))\n",
    "        rm_df = rm_df.drop(\n",
    "            [\n",
    "                \"log_price\",\n",
    "                \"log_price_last\",\n",
    "                \"log_price_first\",\n",
    "                \"interp_beta\",\n",
    "                \"count\",\n",
    "                \"log_price_interp\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "    else:\n",
    "        rm_df[\"price\"] = rm_df.groupby([\"permno\"])[\"price\"].ffill()\n",
    "        rm_df[\"price\"] = rm_df.groupby([\"permno\"])[\"price\"].bfill()\n",
    "\n",
    "    # Add returns\n",
    "    rm_df[\"return\"] = np.where(\n",
    "        rm_df[\"datetime\"].dt.time.astype(str) == \"09:30:00\",\n",
    "        rm_df[\"return\"],\n",
    "        rm_df.groupby([\"permno\"])[\"price\"].pct_change(),\n",
    "    )\n",
    "    rm_df[\"returnx\"] = np.where(\n",
    "        rm_df[\"datetime\"].dt.time.astype(str) == \"09:30:00\",\n",
    "        rm_df[\"returnx\"],\n",
    "        rm_df.groupby([\"permno\"])[\"price\"].pct_change(),\n",
    "    )\n",
    "    # Fix last return for delisting\n",
    "    rm_df[\"return\"] = (1 + rm_df[\"return\"]) * (1 + rm_df[\"dlret\"].fillna(0)) - 1\n",
    "    rm_df = rm_df.drop([\"dlret\"], axis=1)\n",
    "\n",
    "    # Any remaining missing returns will be those originally missing from CRSP\n",
    "    rm_df[\"return\"] = rm_df[\"return\"].fillna(0)\n",
    "\n",
    "    # Add value-weights\n",
    "    rm_df = add_value_weights(rm_df)\n",
    "\n",
    "    return rm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-organization",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instructional-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove existing files\n",
    "# [os.remove(x) for x in tqdm(glob.glob(output_folder + '*'))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70f465f-fd19-430e-a65c-7e5f726583d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f2cf2887a04df6802228ae4c4ba69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mprocess_date\u001b[0;34m(yyyymm)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mhelper_func\u001b[0;34m(df_group)\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29565/434959871.py\u001b[0m in \u001b[0;36mmerge_crsp_taq\u001b[0;34m(date, clean_crsp_date_df, interpolate)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m## Deal with mismatches between TAQ and CRSP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mrm_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_bad_merges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrm_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_minute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m## Interpolate log prices and fill in missing prices/returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29565/434959871.py\u001b[0m in \u001b[0;36mfilter_bad_merges\u001b[0;34m(rm_df, close_hour, close_minute)\u001b[0m\n\u001b[1;32m     42\u001b[0m     rm_df.loc[\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"datetime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrm_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"datetime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_minute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;34m\"price_pct_change\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     ] = np.nan\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m_getter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_create_delegator_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_property_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_setter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m_delegate_property_get\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# maybe need to upcast (ints)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/extension.py\u001b[0m in \u001b[0;36mfget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0mtimestamps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_timestamps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mints_to_pydatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def helper_func(df_group):\n",
    "    return merge_crsp_taq(df_group[0], df_group[1], interpolate = True)\n",
    "\n",
    "def process_date(yyyymm):\n",
    "\n",
    "    # Clean CRSP dataframe for the month\n",
    "    clean_crsp_df = clean_crsp(yyyymm)\n",
    "    \n",
    "    # Go through each date and infill TAQ prices\n",
    "    yyyymm_dates = clean_crsp_df[\"date\"].unique()\n",
    "    df_list = []\n",
    "    for df in tqdm(map(helper_func, clean_crsp_df.groupby([\"date\"]))):\n",
    "        df_list.append(df)\n",
    "        \n",
    "    # Concatenate all files for the month\n",
    "    final_df = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "    # Save\n",
    "    metadata_collector = []\n",
    "    \n",
    "    for day, df in final_df.groupby(['date']):\n",
    "    \n",
    "        mc = []\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        filename = str(pd.to_datetime(day).strftime('%Y%m%d')) + '.parquet'\n",
    "        pq.write_table(table, output_folder + filename, metadata_collector=mc)\n",
    "        mc[-1].set_file_path(filename)\n",
    "        metadata_collector.append(mc)\n",
    "    \n",
    "    return metadata_collector, table.schema\n",
    "\n",
    "process_date('201301')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "postal-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3557e3590f9f46d9b7f3aee489064059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-10:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def helper_func(df_group):\n",
    "    return merge_crsp_taq(df_group[0], df_group[1], interpolate = True)\n",
    "\n",
    "def process_date(yyyymm):\n",
    "\n",
    "    # Clean CRSP dataframe for the month\n",
    "    clean_crsp_df = clean_crsp(yyyymm)\n",
    "    \n",
    "    # Go through each date and infill TAQ prices\n",
    "    yyyymm_dates = clean_crsp_df[\"date\"].unique()\n",
    "    df_list = []\n",
    "    for df in map(helper_func, clean_crsp_df.groupby([\"date\"])):\n",
    "        df_list.append(df)\n",
    "        \n",
    "    # Concatenate all files for the month\n",
    "    final_df = pd.concat(df_list, ignore_index = True)\n",
    "\n",
    "    # Save\n",
    "    metadata_collector = []\n",
    "    \n",
    "    for day, df in final_df.groupby(['date']):\n",
    "    \n",
    "        mc = []\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        filename = str(pd.to_datetime(day).strftime('%Y%m%d')) + '.parquet'\n",
    "        pq.write_table(table, output_folder + filename, metadata_collector=mc)\n",
    "        mc[-1].set_file_path(filename)\n",
    "        metadata_collector.append(mc)\n",
    "    \n",
    "    return metadata_collector, table.schema\n",
    "\n",
    "\n",
    "## Setup \n",
    "# Pyarrow \n",
    "metadata_collector = []\n",
    "\n",
    "# Logging\n",
    "log = []\n",
    "\n",
    "# List of dates\n",
    "yyyymm_list = np.sort(\n",
    "    list(map(lambda x: pd.to_datetime(x).strftime(\"%Y%m\"), pd.date_range('1996-01', '2021-01', freq = '1m')))\n",
    ")\n",
    "\n",
    "# Parallel process each month\n",
    "with Pool(11) as p:\n",
    "    for mc, schema in tqdm(p.imap_unordered(process_date, yyyymm_list), total = len(yyyymm_list)):\n",
    "        \n",
    "        # Add to lists\n",
    "        metadata_collector += mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737733cd-4431-48a4-a355-d7652a81e2f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17220d-3fcb-437c-9a09-6d1dd51fcfea",
   "metadata": {},
   "source": [
    "## Check volatility\n",
    "* Compute fourth moment of returns for each stock\n",
    "* Check stocks that exceed a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a22a54-f988-4000-a29d-e33941dbb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('../../data/proc/clean_prices/*')\n",
    "df_list = []\n",
    "\n",
    "def helper_func(file):\n",
    "    df = pd.read_parquet(file)\n",
    "    df['return4'] = (np.log(1+df['return']))**4\n",
    "    output_df = df.groupby(['permno', 'date'])['return4'].sum().reset_index()\n",
    "    return output_df\n",
    "    \n",
    "with Pool(11) as p:\n",
    "    for output_df in tqdm(p.imap_unordered(helper_func, file_list), total = len(file_list)):\n",
    "        df_list.append(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce8dd8-eda6-43ff-92c3-4db2f793cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data and add stock info\n",
    "concat_df = pd.concat(df_list)\n",
    "stock_info_df = pd.read_feather('../../data/keys/stock_universe.feather')\n",
    "concat_sum_df = (\n",
    "    concat_df.groupby([\"permno\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .merge(stock_info_df.groupby([\"permno\"]).last().reset_index(), on=[\"permno\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea299172-b790-4a63-b2cf-c281f02751b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of r4\n",
    "concat_df['return4_winsor'] = concat_df['return4']\n",
    "concat_df.loc[concat_df['return4'] > 1, 'return4_winsor'] = 1\n",
    "concat_df['return4_winsor'].hist(bins = 30)\n",
    "print(f\"Percent of stocks with very high RQ: {np.mean(concat_df['return4'] > 1):.5%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2d76c-9b3b-4458-bf11-67c70ae48d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stocks with largest RQ\n",
    "concat_sum_df.sort_values(by=\"return4\")[[\"permno\", \"return4\", \"shrcd\", \"exchcd\", \"meq_max\"]].tail(\n",
    "    50\n",
    ").query(\"meq_max > 100000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f56533-50cd-4fb7-904f-bfbc1bf6ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"../../data/proc/clean_prices/*\")\n",
    "df_prices_list = []\n",
    "permno_select = concat_sum_df.query(\"return4 > 10 & meq_max > 100000\")[\"permno\"].unique()\n",
    "\n",
    "\n",
    "def helper_func(file):\n",
    "    df = pd.read_parquet(file, columns = ['datetime', 'permno', 'return', 'value_wt'])\n",
    "    output_df = df.query(\"permno in @permno_select\")\n",
    "    return output_df\n",
    "\n",
    "\n",
    "with Pool(11) as p:\n",
    "    for output_df in tqdm(p.imap_unordered(helper_func, file_list), total=len(file_list)):\n",
    "        df_prices_list.append(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea49bd6-5c4f-4f23-8eea-8e7b019fb846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all data and add stock info\n",
    "concat_prices_df = pd.concat(df_prices_list)\n",
    "# stock_info_df = pd.read_feather('../../data/keys/stock_universe.feather')\n",
    "# concat_sum_df = (\n",
    "#     concat_df.groupby([\"permno\"])\n",
    "#     .sum()\n",
    "#     .reset_index()\n",
    "#     .merge(stock_info_df.groupby([\"permno\"]).last().reset_index(), on=[\"permno\"], how=\"left\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644f2eb-8e69-48ea-8ae1-4d19e5a7b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(permno_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5b14d-b166-4553-bea4-4f4192584415",
   "metadata": {},
   "outputs": [],
   "source": [
    "permno_check = 87658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e21f0-6091-4a08-80af-1a31a482757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_info_df.assign(prc_abs = np.abs(stock_info_df['prc'])).query('permno == @permno_check').plot('dt', 'prc_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f9b2f-297d-43ad-bb99-90bcdc999eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_prices_df.query('permno== @permno_check').set_index('datetime')['return'].dropna().apply(lambda x: np.log(1+x)).cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84187f-daa5-4d11-8bcb-61a53f70208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_prices_df.query('permno== @permno_check').set_index('datetime')['value_wt'].dropna().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb8263-b609-4a6e-9f49-f482dd126004",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_prices_df.query('permno== @permno_check').query('abs(`return`) > 0.9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
