import numpy as npimport loggingimport warningsfrom mlmodels.model import MachineLearningAlgofrom sklearn.exceptions import ConvergenceWarningimport tensorflow as tfimport tensorflowfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras.layers import Dropoutfrom tensorflow.keras.layers import LeakyReLUfrom tensorflow.keras import backend as Kfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpointfrom tensorflow.keras.models import load_modelfrom tensorflow.keras import initializersAdam = tensorflow.keras.optimizers.Adaminitializers = tensorflow.keras.initializers# Handle warnings and logswarnings.simplefilter("error", category=ConvergenceWarning)class Neural_Network(MachineLearningAlgo):    ## Additional parameters    def __init__(        self, hyperparameters, hyperparameter_grid=None, name="NN", n_iter=2    ):        super().__init__(            hyperparameters,            hyperparameter_grid=hyperparameter_grid,            name=name,            n_iter=n_iter,        )        if hyperparameter_grid is None:            # A default value for the hyperparam grid            self.hyperparameter_grid = {}            self.hyperparameter_grid["learning_rate"] = [0.0001, 0.001, 0.005, 0.01]            self.hyperparameter_grid["seed"] = [666]        self.debug = {}    def fit(self, Y_ins, X_ins, X_oos, Y_oos=None, hyperparameters=None, indicator_predict = None):        print(hyperparameters)        if X_ins.shape[1] == 1:            Y_hat = np.zeros(len(X_oos))        else:                        if indicator_predict is None:                if hyperparameters is None:                    self.logger.warning(f"Using default hyperparamters in fit for {self.name}")                    hyperparameters = self.hyperparameters                                                        Initializer = initializers.glorot_normal(seed=hyperparameters["seed"])                                        model = Sequential([                        Dense(64,  kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(24, kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(16,  kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(8, kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(1)                    ])                                st  = f'best_model_seed_{hyperparameters["seed"]}_'                st_2 = f'_input_{X_ins.shape[1]}_'                stN = f'_learning_{hyperparameters["learning_rate"]}.h5'                            callbacks = [EarlyStopping(monitor='val_loss', patience=100),                                     ModelCheckpoint(filepath=(st + st_2 + stN), monitor='val_loss', save_best_only=True)]                                    model.compile(                      loss='mse',                      optimizer=Adam(                    learning_rate=hyperparameters["learning_rate"]),                      metrics=['mse']                      )                            if np.shape(Y_ins)[1] != 1:                    raise NotImplementedError                            # Set seed before fit                np.random.seed(hyperparameters["seed"])                print(X_ins.shape)                # Model                try:                    model.fit(X_ins, Y_ins, epochs=500, verbose=0, callbacks= callbacks,                                                             validation_data=(X_oos, Y_oos))                 except ConvergenceWarning:                    # Don't do anything special                    with warnings.catch_warnings():                        warnings.simplefilter("ignore")                        self.logger.debug("NN Convergence Warning")                        model.fit(X_ins, Y_ins, epochs=500, verbose=0, callbacks= callbacks,                                                             validation_data=(X_oos, Y_oos))                except Exception as e:                    raise (e)                        st  = f'best_model_seed_{hyperparameters["seed"]}_'            st_2 = f'_input_{X_ins.shape[1]}_'            stN = f'_learning_{hyperparameters["learning_rate"]}.h5'                                    saved_model = load_model(st + st_2 + stN)            # Predict            Y_hat = saved_model.predict(X_oos)        # self.debug['sklearn_model'] = sklearn_model        fit_params = {}                return Y_hat, fit_paramsclass Neural_Network_simple(MachineLearningAlgo):    ## Additional parameters    def __init__(        self, hyperparameters, hyperparameter_grid=None, name="NN_Simple", n_iter=2    ):        super().__init__(            hyperparameters,            hyperparameter_grid=hyperparameter_grid,            name=name,            n_iter=n_iter,        )        if hyperparameter_grid is None:            # A default value for the hyperparam grid            self.hyperparameter_grid = {}            self.hyperparameter_grid["learning_rate"] = [0.0001, 0.001, 0.005, 0.01]            self.hyperparameter_grid["seed"] = [666, 1]        self.debug = {}    def fit(self, Y_ins, X_ins, X_oos, Y_oos=None, hyperparameters=None, indicator_predict = None):        print(hyperparameters)        if X_ins.shape[1] == 1:            Y_hat = np.zeros(len(X_oos))        else:                        if indicator_predict is None:                if hyperparameters is None:                    self.logger.warning(f"Using default hyperparamters in fit for {self.name}")                    hyperparameters = self.hyperparameters                                                        Initializer = initializers.glorot_normal(seed=hyperparameters["seed"])                                        model = Sequential([                        Dense(16,  kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(8, kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(4, kernel_initializer=Initializer),                        LeakyReLU(alpha = 0.1),                        Dropout(0.2),                        Dense(1)                    ])                                st  = f'best_model_seed_{hyperparameters["seed"]}_'                st_2 = f'_input_{X_ins.shape[1]}_'                stN = f'_learning_{hyperparameters["learning_rate"]}.h5'                            callbacks = [EarlyStopping(monitor='val_loss', patience=100),                                     ModelCheckpoint(filepath=(st + st_2 + stN), monitor='val_loss', save_best_only=True)]                                  model.compile(                      loss='mse',                      optimizer=Adam(                    learning_rate=hyperparameters["learning_rate"]),                      metrics=['mse']                      )                               if np.shape(Y_ins)[1] != 1:                    raise NotImplementedError                            # Set seed before fit                np.random.seed(hyperparameters["seed"])                            # Model                try:                    model.fit(X_ins, Y_ins, epochs=500, verbose=0, callbacks= callbacks,                                                             validation_data=(X_oos, Y_oos))                 except ConvergenceWarning:                    # Don't do anything special                    with warnings.catch_warnings():                        warnings.simplefilter("ignore")                        self.logger.debug("NN Convergence Warning")                        model.fit(X_ins, Y_ins, epochs=500, verbose=0, callbacks= callbacks,                                                             validation_data=(X_oos, Y_oos))                except Exception as e:                    raise (e)                        st  = f'best_model_seed_{hyperparameters["seed"]}_'            st_2 = f'_input_{X_ins.shape[1]}_'            stN = f'_learning_{hyperparameters["learning_rate"]}.h5'                                    saved_model = load_model(st + st_2 + stN)            # Predict            Y_hat = saved_model.predict(X_oos)        # self.debug['sklearn_model'] = sklearn_model        fit_params = {}                return Y_hat, fit_params        def predict(self, X):        pass